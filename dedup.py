#!/usr/bin/env python
import argparse
import hashlib
import os
import functools
import toml
import shutil
import re

def main():
    parser = argparse.ArgumentParser(prog="dedup")
    subparsers = parser.add_subparsers(dest="command")

    parser_gen = subparsers.add_parser("generate")
    parser_gen.add_argument("directories", nargs="+", help="The directories to generate a duplication list for")
    parser_gen.add_argument("-o", "--output", help="generate dedup info in this file")
    
    parser_finalize = subparsers.add_parser("finalize")
    parser_finalize.add_argument("-d", "--dup-file", default="dedup.txt", help="dedup.txt by default. The file generated by 'dedup generate {directory}' containing duplicated directories")
    parser_finalize.add_argument("-m", "--move", default=None, help="Instead of deleting duplicates, move to this directory")
    parser_finalize.add_argument("-r", "--remove-dir", action="store_true", help="Remove empty directories after move/delete")
    parser_finalize.add_argument("-p", "--preserve-dir", action="store_true", help="Preserve directory output on move")
    parser_finalize.add_argument("-v", "--verbose", action="store_true")
    parser_finalize.add_argument("-t", "--dry-run", action="store_true", help="Do not perform changes on the filesystem. Only print verbose info")

    args = parser.parse_args()

    if args.command == "generate":
        handle_generate(args.directories, args.output)
    elif args.command == "finalize":
        if args.dry_run:
            args.verbose = True
        handle_finalize(args.dup_file, args.move, args.remove_dir, args.preserve_dir, args.verbose, args.dry_run)

def handle_finalize(dup_file="dedup.txt", move=None, remove_dir=False, preserve_dir=False, verbose=False, dry_run=False):
    with open(dup_file, "r") as f:
        dedup = toml.load(f)
    
    for hash, duplicates in dedup.items():
        removes = duplicates["remove"]

        for remove in removes:  
            # if move instead of delete, move while not overwriting files with same name
            if move:
                if preserve_dir:
                    move_path = os.path.join(move, remove)
                    if not dry_run:
                        os.makedirs(os.path.dirname(move_path), exist_ok=True)
                else:
                    move_path = os.path.join(move, os.path.basename(remove))



                # if path exists, come up with unique path
                while os.path.exists(move_path):
                    dedup_pattern = r"dedup (\d+)(\.\w+)?"
                    match = re.search(dedup_pattern, move_path)
                    if match:
                        move_path = re.sub(dedup_pattern, f"dedup {str(int(match.group(1))+1)}{match.group(2)}", move_path)
                    else:
                        name, ext = os.path.splitext(remove)
                        move_path = os.path.join(move, f"{name} dedup 1{ext}")
                        

                if verbose:
                    print(f"MOVE {remove} ➡️ {move_path}")

                if not dry_run:
                    shutil.move(remove, move_path)
            else:
                if verbose:
                    print(f"DELETE {remove}")
                
                if not dry_run:
                    os.remove(remove)
                
                if remove_dir and not dry_run:
                    # remove empty parent dirs
                    dir = os.path.dirname(remove)
                    while not os.listdir(dir):
                        print(f"RMDIR {dir}")
                        os.rmdir(dir)
                        dir = os.path.dirname(dir)
    
def handle_generate(directories, output="./dedup.txt"):
    dedup = {}
    duplicates = {}
    for directory in directories:
        # put a list of all the files with same hashes as value of key hash
        # get all the lists of size > 1, with list[0] being the first found file in duplicate list
        # (so size 1 means no duplicates)
        for root, _, files in os.walk(directory):
            for file in files:
                path = os.path.join(root, file)
                with open(path, "rb") as f:
                    hash = hashlib.sha256(f.read()).hexdigest()

                    if hash in dedup:
                        duplicates[hash].append(path)
                    else:
                        dedup[hash] = path
                        duplicates[hash] = [path]

    duplicates = {k:v for k,v in duplicates.items() if len(v) > 1}

    with open(output, "w") as output:
        toml_dict = {}
        for hash, duplicate in duplicates.items():
            # shortest is usually original
            # we want it to be sorted by directory, but we also want
            # the shortest name value
            duplicate_copy = duplicate.copy()
            duplicate_copy.sort(key=lambda item: len(os.path.basename(item)))

            keep = duplicate_copy[0]
            remove = list(filter(lambda d: d != keep, duplicate))

            toml_dict[hash] = {}
            toml_dict[hash]["keep"] = keep
            toml_dict[hash]["remove"] = remove
        
        toml.dump(toml_dict, output)

    
if __name__ == "__main__":
    main()