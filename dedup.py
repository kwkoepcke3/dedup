#!/usr/bin/env python
import argparse
import hashlib
import os
import functools
import toml
import shutil
import re

def main():
    parser = argparse.ArgumentParser(prog="dedup")
    subparsers = parser.add_subparsers(dest="command")

    parser_gen = subparsers.add_parser("generate")
    parser_gen.add_argument("directory", help="The directory to generate a duplication list for")
    
    parser_finalize = subparsers.add_parser("finalize")
    parser_finalize.add_argument("-d", "--dup_file", default="dedup.txt", help="dedup.txt by default. The file generated by 'dedup generate {directory}' containing duplicated directories")
    parser_finalize.add_argument("-m", "--move", default=None, help="Instead of deleting duplicates, move to this directory")

    args = parser.parse_args()

    if args.command == "generate":
        handle_generate(args.directory)
    elif args.command == "finalize":
        handle_finalize(args.dup_file, args.move)

def handle_finalize(dup_file="dedup.txt", move=None):
    with open(dup_file, "r") as f:
        dedup = toml.load(f)
    
    for hash, duplicates in dedup.items():
        removes = duplicates["remove"]

        for remove in removes:  
            # if move instead of delete, move while not overwriting files with same name
            if move:
                move_path = os.path.join(move, os.path.basename(remove))

                # if path exists, come up with unique path
                while os.path.exists(move_path):
                    dedup_pattern = r"dedup (\d+)(\.\w+)?"
                    match = re.search(dedup_pattern, move_path)
                    if match:
                        print(f"match {move_path}")
                        move_path = re.sub(dedup_pattern, f"dedup {str(int(match.group(1))+1)}{match.group(2)}", move_path)
                    else:
                        print(f"no match {move_path}")
                        name, ext = os.path.splitext(os.path.basename(remove))

                        move_path = os.path.join(move, f"{name} dedup 1{ext}")
                        

                shutil.move(remove, move_path)
            else:
                os.remove(remove)

def handle_generate(directory):
    dedup = {}
    duplicates = {}

    # put a list of all the files with same hashes as value of key hash
    # get all the lists of size > 1, with list[0] being the first found file in duplicate list
    # (so size 1 means no duplicates)
    for root, _, files in os.walk(directory):
        for file in files:
            path = os.path.join(root, file)
            with open(path, "rb") as f:
                hash = hashlib.sha256(f.read()).hexdigest()

                if hash in dedup:
                    duplicates[hash].append(path)
                else:
                    dedup[hash] = path
                    duplicates[hash] = [path]

    duplicates = {k:v for k,v in duplicates.items() if len(v) > 1}

    with open("dedup.txt", "w") as output:
        toml_dict = {}
        for hash, duplicate in duplicates.items():
            # shortest is usually original
            duplicate.sort(key=lambda item: len(item))
            toml_dict[hash] = {}
            toml_dict[hash]["keep"] = duplicate[0]
            toml_dict[hash]["remove"] = duplicate[1:]
        
        toml.dump(toml_dict, output)

    
if __name__ == "__main__":
    main()